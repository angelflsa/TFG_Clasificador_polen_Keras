{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador Olivo/gramíneas a partir de una red pre-entrenada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.layers import  Convolution2D, MaxPooling2D, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo VGG16:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el modelo VGG16: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo contiene los pesos obtenidos con el pre-entrenamiento en ImageNet. Por defecto, establece una imagen de entrada de 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 480, 640, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 480, 640, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 480, 640, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 240, 320, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 240, 320, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 240, 320, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 120, 160, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 120, 160, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 120, 160, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 120, 160, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 60, 80, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 60, 80, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 60, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 60, 80, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 30, 40, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 30, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 30, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 30, 40, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 15, 20, 512)       0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16 = applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(480, 640, 3), classes=2)#Cargamos la red\n",
    "vgg16.summary()#Muestra el contenido de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha pasado como argumento include_top=False. La red vgg16 que aporta keras contiene por defecto una última capa de predicción de mil neuronas (para clasificar mil clases diferentes). La red que se busca solo clasificará dos clases diferentes. Con include_top=False se elimina esta última capa. En la siguiente línea de código se carga la red al completo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificación de la última capa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se diseña la capa de salida para realizar la clasificación. Consytará de:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa GlobalAveragePooling2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capa tipo Dense de 1000 neuronas y activación rectificador (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = vgg.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(300, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 3 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=vgg.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congelación de capas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha cargado la red pre-entrenada vgg16, con los pesos de Imagenet. De esta forma las capas convolucionales y pooling que la forman actuarán como diferenciadoras de características, que serán usadas por la capa final que se ha incluido para realizar la clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, se va a usar una red que ya ha 'aprendido' a clasificar en otros problemas, y se va a adaptar para el problema que aquí se trata (TRANSFER LEARNING)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es por esto que no es necesario entrenar todas las capas de la red, y solo se entrenarán las 3 capas que se han añadido, congelando las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:19]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[19:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que antes los parámetros no entrenables eran nulos, y ahora han pasado a ser 14,714,688."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamiento de las imágenes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necesita adaptar el conjunto de imágenes para poder ser tratado por el modelo. Para ello se van a usar las funciones flow_from_directory() y ImageDataGenerator()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se almacenan en variables los directorios en los que se encuentran las imágenes\n",
    "data_entrenamiento = './data_polen/entrenamiento'\n",
    "data_validacion = './data_polen/validacion'\n",
    "\n",
    "#Parámetros importantes:\n",
    "epocas=10\n",
    "longitud, altura = 640, 480\n",
    "batch_size = 18 #Imágenes a procesar en cada paso\n",
    "pasos = 10\n",
    "validation_steps = 8 #Imágenes de validación que se pasan al final de cada época\n",
    "clases = 2\n",
    "lr = 0.00085 #Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la función flow_from_directory() se pre-procesan las imágenes que se encuentran en los directorios previamente declarados. Además se le puede pasar como parámetros el tamaño al que se redimensionan las imágenes, o el algoritmo de interpolación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a la función ImageDataGenerator() se aplica al set de entrenamiento mecanismos de DATA ARGUMENTATION como inclinar, hacer zoom o invertir las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "{'gramineas': 0, 'olivo': 1}\n"
     ]
    }
   ],
   "source": [
    "###Procesamiento del conjunto de entrenamieto:\n",
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255, \n",
    "    shear_range=0.2, #Inclina las imágenes\n",
    "    zoom_range=0.2, #Zoom a algunas imágenes\n",
    "    horizontal_flip=True) #Invierte imágenes para distinguir direcionalidad\n",
    "\n",
    "###Procesamiento del conjunto de validación:\n",
    "#No es necesario inclinar, hacer zoom ni invertir las imágenes.\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "###Generación del conjunto de entrenamieto:\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical') #Se busca una clasificación categórica\n",
    "\n",
    "###Generación del conjunto de validación:\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "print(entrenamiento_generador.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del modelo CNN: función pérdida y optimizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=optimizers.Adam(lr=lr),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Angel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 587s 196s/step - loss: 0.7519 - acc: 0.5926 - val_loss: 0.6549 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 591s 197s/step - loss: 0.6557 - acc: 0.5741 - val_loss: 0.5965 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 598s 199s/step - loss: 0.5818 - acc: 0.9259 - val_loss: 0.5805 - val_acc: 0.6500\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 598s 199s/step - loss: 0.5165 - acc: 0.7407 - val_loss: 0.5756 - val_acc: 0.6500\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 595s 198s/step - loss: 0.4921 - acc: 0.6852 - val_loss: 0.5441 - val_acc: 0.6500\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 593s 198s/step - loss: 0.4563 - acc: 0.8704 - val_loss: 0.4522 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 589s 196s/step - loss: 0.4071 - acc: 0.9444 - val_loss: 0.4071 - val_acc: 0.9500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 591s 197s/step - loss: 0.3805 - acc: 0.9444 - val_loss: 0.3884 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 583s 194s/step - loss: 0.3485 - acc: 0.9444 - val_loss: 0.3867 - val_acc: 0.8500\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 586s 195s/step - loss: 0.3219 - acc: 0.9259 - val_loss: 0.3533 - val_acc: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24daa728828>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    entrenamiento_generador,\n",
    "    steps_per_epoch=len(entrenamiento_generador),\n",
    "    epochs=epocas,\n",
    "    validation_data=validacion_generador,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de la función pérdida:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) se encarga de calcular la función de pérdida dados unos datos de entrada y el nivel de aciertos del modelo para una muestra dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validacion_generador, steps=pasos, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo entrenado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para no tener que entrenar la red neuronal creada cada vez que se quiera usar, se crea un archivo donde se guarda el modelo creado, y otro donde se guardan los pesos obtenidos para las neuronas después del entranmiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir = './vgg16_OliGram_Prueba/'\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "model.save('./vgg16_OliGram_Prueba/modelo_vgg16.h5')#Se guarda la estructura de la cnn\n",
    "model.save_weights('./vgg16_OliGram_Prueba/pesos_vgg16.h5')#Se guardan los pesos de la cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas de clasificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función load_image() transforma de forma interna las imágenes, tomando como argumento las dimensiones que admiten las arquitecturas implementadas y un método de interpolación. Se recomienda usar métodos de interpolación como bicubic o lanczos, frente a nearest que viene por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Función predicción:\n",
    "def predict(file):\n",
    "  x = load_img(file, target_size=(altura, longitud))\n",
    "  x = img_to_array(x)\n",
    "  x = np.expand_dims(x, axis=0) #Zero mean pre-processing, normalize data.\n",
    "  array = model.predict(x)\n",
    "  print(array)  \n",
    "  result = array[0]\n",
    "  print(result)\n",
    "  answer = np.argmax(result)\n",
    "  print(answer)  \n",
    "  if answer == 0:\n",
    "    print(\"pred: Gramineas\")\n",
    "  elif answer == 1:\n",
    "    print(\"pred: Olivo\")\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41358513 0.5864149 ]]\n",
      "[0.41358513 0.5864149 ]\n",
      "1\n",
      "pred: Olivo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('olivo1.jpg') #Imagen de olivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9967051e-01 3.2954017e-04]]\n",
      "[9.9967051e-01 3.2954017e-04]\n",
      "0\n",
      "pred: Gramineas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('graminea2.jpg') #Imagen de graminea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9773102  0.02268984]]\n",
      "[0.9773102  0.02268984]\n",
      "0\n",
      "pred: Gramineas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('graminea4.jpg') #Imagen de graminea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01226435 0.9877357 ]]\n",
      "[0.01226435 0.9877357 ]\n",
      "1\n",
      "pred: Olivo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('olivo5.jpg') #Imagen de olivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4479748 0.5520252]]\n",
      "[0.4479748 0.5520252]\n",
      "1\n",
      "pred: Olivo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('olivo2.jpg') #Imagen de olivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
